---
title: "Clustering methods"
author: "Denaldo Lapi, Francesco Aristei, Samy Chouity"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  word_document:
    toc: yes
    toc_depth: '2'
    
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
subtitle: Hierarchical, Partitioning, and Model-based Clustering
toc-title: Outline
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Delete all the possible objects of R that could have been left in memory:
```{r, include=TRUE}
rm(list = ls())
```

At first, let's load the libraries we'll need:

* *dplyr* and *ggplot2* will help us with data manipulation and graphs, respectively.
* *kableExtra* will help us to print beautiful tables.
* *gridExtra* for arranging the layout of the graphs.
* *stats* computes hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it ( _hclust_).
* *cluster* to apply the _agnes_ (agglomerative hierarchical clustering) and the _diana_ (divisive hierarchical clustering).
* *gplots* computes heatmaps for visualizing the distance matrix.
* *factoextra* for MVA methods and graph the clustering structures.
* *FactoMineR* computes hierarchical clustering on principal components.

```{r message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(stats)
library(gplots)
library(cluster)
library(factoextra)
library(FactoMineR)
```

## Exploratory data analysis

The dataset we are going to analyze is about prostate cancer. It consists of p=12 variables of mixed type (8 continuous and 4 categorical) measured on a group of n=50 prostate cancer patients. These patients have either stage 3 or stage 4 prostate cancer.


Let's load the data from the R image 'Prostate.RData'.

```{r}
load("Prostate.RData")
```

Check dimension:

```{r}
dim(Prostate)
```

We have 50 rows and 12 columns

Check the structure:

```{r}
str(Prostate)
```

We can print a portion (a sample of 10) of the table using kable and the pipe operator:

```{r}
Prostate %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Prostate cancer data set (sample of 20)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Let's now visualize some basic statistics on each of the data frame's columns with *summary*:

```{r}
Prostate %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Prostate cancer data set") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Since clustering methods we analyzed work by considering a continuous domain, i.e. continuous variables, we'll remove the categorical ones.
Another possibility could be to encode them in some numeric way, but we should take care of the distance measure to use in the hierarchical and k-means algorithms, which is difficult to express in case of categorical variables (euclidean distances doesn't make a lot of sense in the case of categorical variables, we should considere other types of distances).

Let's select only the numerical columns:

```{r}
library(purrr)
Prostate_reduced = Prostate[, c(1,2,5,6,8,9,10,11)]
```

Let's now check again the structure:

```{r}
str(Prostate_reduced)
```

And the dimensions:

```{r}
dim(Prostate_reduced)
```


### Check for missing values

Let's check for NA values:

```{r}
colSums((is.na((Prostate_reduced))))
```

There are no missing values!

### Outliers

One way to check for multivariate outliers is to use the [Malhanobis' distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) . It can be thought of as a metric for estimating how far each observation is from the center of all the variables' distributions (i.e. the centroid in the multivariate space).

We'll use the *chemometrics* package, which contains a function ('Moutlier') for calculating and plotting both the "Mahalanobis'" distance and a robust version of the "Mahalanobis'" distance.

At first, let's calculate the "Mahalanobis'" distances using the 'Moutlier' function, to which we provide as parameters the numeric data frame, the quantile cutoff point beyond which we want to identify points as outliers, and whether or not we want a plot:

```{r}
#install.packages("chemometrics")
library(chemometrics)
md <- Moutlier(Prostate_reduced, quantile = 0.975, plot=FALSE)
```

The function returns the cutoff value for the outliers:

```{r}
md$cutoff
```

We simply use the 'which' function to identify which cases are outliers according to the 'cutoff' value and in this way we obtain the outliers' indexes:

```{r}
outliers <- which(md$md > md$cutoff)
head(outliers, 10) # show first 10 outliers according to Malhanobis distance
```

Another approach for identifying multivariate outliers (non-parametric approach) is to use the LOF ("local outlier factor") algorithm, which identifies density-based local outliers.

The algorithm we are going to use (from the package [DDoutlier](https://rdrr.io/cran/DDoutlier/man/LOF.html)) computes a local density for observations with a given k-nearest neighbors (we choose k = 5). This local density is compared to the density of the respective nearest neighbors, resulting in the local outlier factor.

Therefore, the function returns a vector of LOF scores for each observation: the greater the LOF, the greater the outlierness of the data point.

```{r}
#install.packages('DDoutlier')
library("DDoutlier")
lof <- LOF(Prostate_reduced, k = 5) # outlier score with a neighborhood of 5 points
```

We can show the lof scores for the 5 first observations:

```{r}
head(lof)
```

We can see and visualize the distribution of outlier scores:

```{r}
summary(lof) # some statistics
hist(lof)
```

It could be useful to plot also the sorted LOF scores:

```{r}
plot(sort(lof), type = "l",  main = "LOF (K = 5)",
  xlab = "Points sorted by LOF", ylab = "LOF")
```

Looks like outliers start around a LOF value of 2.0.

Let's show the indexes for 5 most outlying observations

```{r}
lof_with_names = lof
names(lof_with_names) <- 1:nrow(Prostate)
sort(lof_with_names, decreasing = TRUE)[1:5]
```

Let's first find the indexes of the outliers with a lof score above 2.0:

```{r}
outliers_lof <- which(lof > 2.0)
```

Number of detected outliers:

```{r}
length(outliers_lof)
```

```{r}
outliers_lof
```


We will simply remove the found outliers (found with lof) from the dataset (taking into account that k-means algorithm is very sensible to outliers):

```{r}
Prostate_no_outliers = Prostate_reduced[-outliers_lof,] 
```

Let's now check again the dimensions:

```{r}
dim(Prostate_no_outliers)
```

The outliers have been correctly removed!


### Check variables distribution

Check variables correlation:

```{r}
library(GGally)
ggpairs(Prostate_no_outliers,
        title="Correlation matrix. Phoneme data")
```

Let's now check the univariate distribution of each variable.

We'll use the density plot:

```{r}
library(gridExtra)
library(ggplot2)
g1 <- ggplot(Prostate_no_outliers, aes(x=age)) + geom_density(alpha=0.8)
g2 <- ggplot(Prostate_no_outliers, aes(x=wt)) + geom_density(alpha=0.8)
g3 <- ggplot(Prostate_no_outliers, aes(x=sbp)) + geom_density(alpha=0.8)
g4 <- ggplot(Prostate_no_outliers, aes(x=dbp)) + geom_density(alpha=0.8)
g5 <- ggplot(Prostate_no_outliers, aes(x=hg)) + geom_density(alpha=0.8)
g6 <- ggplot(Prostate_no_outliers, aes(x=sz)) + geom_density(alpha=0.8)
g7 <- ggplot(Prostate_no_outliers, aes(x=sg)) + geom_density(alpha=0.8)
g8 <- ggplot(Prostate_no_outliers, aes(x=ap)) + geom_density(alpha=0.8)

grid.arrange(g1,g2, nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7,g8,nrow=1)
```

A better approach for a visual inspection is the *Q-Q plot*, which shows the distribution of the data against the expected normal distribution. In particular, for normally distributed data, observations should lie approximately on a straight line. If the data is non-normal, the points form a curve that deviates markedly from a straight line. Let's perform such plot for each predictor, by using the library *ggpubr*:


```{r}
library(gridExtra)
library(ggpubr)
library(ggplot2)

g1 <- ggqqplot(Prostate_no_outliers, x="age", col=2, ggtheme = theme_gray(), title = "age Q-Q plot")
g2 <- ggqqplot(Prostate_no_outliers, x="wt", col=2, ggtheme = theme_gray(), title = "wt Q-Q plot")
g3 <- ggqqplot(Prostate_no_outliers, x="sbp", col=2, ggtheme = theme_gray(), title = "sbp Q-Q plot")
g4 <- ggqqplot(Prostate_no_outliers, x="dbp", col=2, ggtheme = theme_gray(), title = "dbp Q-Q plot")
g5 <- ggqqplot(Prostate_no_outliers, x="hg", col=2, ggtheme = theme_gray(), title = "hg Q-Q plot")
g6 <- ggqqplot(Prostate_no_outliers, x="sz", col=2, ggtheme = theme_gray(), title = "sz Q-Q plot")
g7 <- ggqqplot(Prostate_no_outliers, x="sg", col=2, ggtheme = theme_gray(), title = "sg Q-Q plot")
g8 <- ggqqplot(Prostate_no_outliers, x="ap", col=2, ggtheme = theme_gray(), title = "ap Q-Q plot")
grid.arrange(g1,g2, nrow=1); grid.arrange(g3,g4,nrow=1);grid.arrange(g5,g6,nrow=1); grid.arrange(g7,g8,nrow=1)

```

The plots above clearly show that most of the variables does not follow a normal distribution.

To have more precise insights, we can apply the normality [Shapiro-Wilk normality test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) to each variable:

```{r}

print(shapiro.test(Prostate_no_outliers$age))
print(shapiro.test(Prostate_no_outliers$wt))
print(shapiro.test(Prostate_no_outliers$sbp))
print(shapiro.test(Prostate_no_outliers$dbp))
print(shapiro.test(Prostate_no_outliers$hg))
print(shapiro.test(Prostate_no_outliers$sz))
print(shapiro.test(Prostate_no_outliers$sg))
print(shapiro.test(Prostate_no_outliers$ap))

```

The low p-values (\<0.05) for the variables 'age', 'dbp', 'sz', 'ap', 'sg' reject the null hypotheses as we expected from the plots.


However, clustering techniques related to **distance-based methods** (e.g. hierarchical clustering & k-means) **has nothing to do with whether the variables** belong to some known distribution such as the **normal distribution**. For this reason we do not apply any transformation to change variables distributions.


### Scaling

The **units** of the variables might have **an influence** in the clustering results for what regards clustering methods based on distances such as k-means and the hierarchical one. 
We do not want the results of those clustering methods to depend on a variable with very large units, so we **scale (standardize) each variable**:

```{r}
Prostate_no_outliers_sc <- Prostate_no_outliers %>%
        mutate_if(is.numeric, scale)
```

We can print a sample of the scaled table to see the new units:

```{r}
Prostate_no_outliers_sc %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Prostate cancer data set (sample of 20)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Let's print again the basic statistics:

```{r}
Prostate_no_outliers_sc %>% 
  summary(.) %>% 
  kbl(caption = "Basic statistics. Prostate cancer data set") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

As we can see, we have now zero mean.


# Hierarchical clustering

In this section we are going to apply various hierarchical clustering methods that allow to draw dendrograms, and then we'll compare their results.

The first experiment that we are going to run is to differentiate agglomerative and divisive clustering:

* Agglomerative Clustering: also known as a bottom-up approach, starts with singletons and then agglomerates them based on their distances until having a group containing the whole set of elements.
* Divisive Clustering: also known as a top-down approach, starts with one group (or cluster) containing every element, then divides them using a splitting method until reaching singletons clusters.

An important parameter of the algorithm is the distance measure to apply for calculating the distances between points. 
A further parameter is needed to calculate the distance between groups. We'll explore all of them in the following.

## Agglomerative hierarchical clustering
To experiment the agglomerative hierarchical clustering, we'll use the *agnes* package.
We'll compute agglomerative hierarchical clustering using the euclidean distance for the points, while we'll compare 4 different approaches for what regards groups distances:
- Simple linkage
- Average linkage
- Complete linkage
- Ward criterion

Since *agnes* returns also the 'agglomerative coefficient', we'll compare the solutions obtained using the various distance approaches we mentioned above using the agglomerative coefficient that represents the amount of clustering structure found (values closer to 1 suggest a strong clustering structure).

Let's compare the 4 approaches, by using the euclidean distance among points:

```{r, include=TRUE}
# vector of methods to compare
m <- c("average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
 
# function to compute coefficient
ac <- function(x) {
  agnes(Prostate_no_outliers_sc, method = x)$ac
}
library(purrr)
map_dbl(m, ac)
```

Among the 4 distance computation methods, Ward's method gave us the highest agglomerative coefficient.
Therefore, we can inspect its dendrogram:

```{r, include=TRUE}
agnes.ward <- agnes(Prostate_no_outliers_sc, method = "ward")
agnes_dend <- agnes.ward %>% as.dendrogram
pltree(agnes.ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

### Cutting the tree

In hierarchical clustering we should decide where to cut the dendrogram, in order to obtain an actual assignment of observations to clusters: this means that we should decide the optimal number of clusters.

We can do that with the function *cutree* which cuts a tree into several groups either by specifying the desired number of groups or the cut height.

One typical approach consists in cutting the tree where the branches are quite long: in the above dendrogram we can see that this happens at a height between 8 and 9, where we have 3 clusters. 
Let's then perform the cutting:

```{r, include=TRUE}
(clust <- cutree(agnes.ward, k = 3)) 
```

In order to graph those clusters, we can use the function *fviz_cluster* from the package _factorextra_:

```{r, include=TRUE}
fviz_cluster(list(data = Prostate_no_outliers_sc, cluster = clust))
```

We can also depict the clusters within the dendrogram in this way:

```{r, include=TRUE}
pltree(agnes.ward, hang=-1, cex = 0.6)
rect.hclust(agnes.ward, k = 3, border = 2:5)
```

## Divisive hierarchical clustering
After having performed hierarchical clustering using the agglomerative approach, it makes sense to apply the divisive option: To do so we use the _diana_  algorithm.
The function _diana_ works in a similar way to agnes with 2 small differences: there is *no method argument here*, and instead of the agglomerative coefficient, we have a division coefficient.

```{r, include=TRUE}
library(dendextend)
diana_dend <- diana(Prostate_no_outliers_sc) %>% as.dendrogram
diana_dend <- color_branches(diana_dend, k = 2)

```

We proceed now in computing the divisive coefficient: if it is closer to one suggests stronger group distinctions.
The divisive coefficient is:

```{r, include=TRUE}
diana.hc <- diana(Prostate_no_outliers_sc)
diana.hc$dc
```

We obtain a value slightly lower than the agglomerative coefficient obtained with the agglomerative approach (0.8309064).

Finally, we plot of the dendogram:
```{r, include=TRUE}
pltree(diana.hc, cex = 0.6, hang = -1, main = "Dendrogram of diana")
```

### Cutting the tree

By looking at the length of the branches, the above dendrogram suggests to cut at a height around 6, which gives us 4 clusters.

Let's then perform the cutting:

```{r, include=TRUE}
(clust <- cutree(diana_dend, k = 4)) 
```

We can graph the clusters:

```{r, include=TRUE}
fviz_cluster(list(data = Prostate_no_outliers_sc, cluster = clust))
```

It seems that the 4th cluster is not well representative of a class of individuals: indeed it is formed only by 2 observations. 
We therefore re-perform the cutting with k=3: 

```{r, include=TRUE}
(clust <- cutree(diana_dend, k = 3)) 
```

We can graph again the clusters:

```{r, include=TRUE}
fviz_cluster(list(data = Prostate_no_outliers_sc, cluster = clust))
```

The plots seems now much more consistent, with a small overlapping between clusters 2 and 3.

We can also depict the clusters within the dendrogram in this way:

```{r, include=TRUE}
pltree(diana.hc, hang=-1, cex = 0.6)
rect.hclust(diana.hc, k = 3, border = 2:5)
```


## Comparing agglomerative and divisive methods

We can compare the partitions obtained throught the two approaches by using a _tanglegram_ which makes us able to compare the two dendograms by drawing links between them:

```{r, include=TRUE}
library(dendextend)
diana_dend <- diana.hc %>% as.dendrogram
tanglegram(agnes_dend, diana_dend)
```

A better way to visualize the differences among the partitions obtained with the 2 algorithms, is to visualize the 2 dendrograms one below the other:

```{r}
pltree(agnes.ward, hang=-1, cex = 0.6)
rect.hclust(agnes.ward, k = 3, border = 2:5)

pltree(diana.hc, hang=-1, cex = 0.6)
rect.hclust(diana.hc, k = 3, border = 2:5)
```

The 3 clusters are slighlty different between the methods: for instance we can see that the 'green' cluster is bigger in 'diana' with respect to 'agnes'.


## Hierarchical clustering on principal components using _HCPC_

Another approach consists in applying firstly a PCA to the continuous dataset, and then a clustering method to the obtained reduced representation.

In this approach, the PCA step can be considered as a denoising step which can lead to a more stable clustering. This might be useful in our case since we are doing clustering by considering 8 variables.

The algorithm of the HCPC method, as implemented in the _FactoMineR_ package, can be summarized as follow:

1. _Compute PCA_: we retain 3 dimensions

```{r, include=TRUE}
res.pca <- PCA(Prostate_no_outliers_sc, ncp = 3, graph = FALSE)
fviz_pca_biplot(res.pca) +
       theme_minimal()
```

2. _Compute hierarchical clustering_: Hierarchical clustering is performed using the Ward's criterion on the selected principal components. Ward criterion is used in the hierarchical clustering because it is based on the multidimensional variance like principal component analysis.

```{r, include=TRUE}
res.hcpc <- HCPC(res.pca, graph = FALSE)
```

The function _HCPC_ returns a list containing:

* *data.clust*: The original data with a supplementary column called class containing the partition.
* *desc.var*: The variables describing clusters
* *desc.ind*: The more typical individuals of each cluster
* *desc.axes*: The axes describing clusters

Therefore, the *HCPC* function already performs an initial partitioning. 
To know the number of clusters:

```{r, include=TRUE}
unique(res.hcpc$data.clust$clust)
```

The method returns the same number of clusters we found with the previous approaches (3).
Thus, let's display the original data with cluster assignments:

```{r, include=TRUE}
res.hcpc$data.clust %>%
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Cluster allocation (sample of 10)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

In the table above, the last column contains the cluster assignments.

To display quantitative variables that describe the most each cluster:

```{r, include=TRUE}
res.hcpc$desc.var$quanti
```

Let's focus on columns "Mean in category" (the average within the corresponding group), "Overall Mean" (the overall mean in the data set), "p.value" (<0.05 means that there is significant difference between Overall mean and the mean within the corresponding group).

From the output above, it can be seen that:

* Cluster 1 is characterized by positive values of variables 'ap', 'sg', 'sz';
* For what regard cluster 2, we have pretty high p-values with respect to the other clusters, but still we have p-values < 5% and therefore we can draw some conclusions. What we can say is that this cluster is characterized by negative (standardized) values of each one of the variables: for instance, with respect to cluster 1, in cluster 2 the variable 'sz' has a mean in the category of -0.37, while we have a positive value of the mean for the same variable on cluster 1. The same reasoning can be applied for the other variables variables.
* Cluster 3 has, on overall positive values for the varaible 'dbp' (which has negative mean in cluster 1 and 2); it has also positive value for variable 'sbp', which is also significant for cluster 2 where it has a negative mean. Also variable 'hg' has positive value in this cluster, and it's also significant in cluster 2 with negative mean value. Varaible 'wt' has also positive value in this cluster. So, overall cluster 3 is characterize by positive mean values (standardized) of the variables 'dbp', 'sbp', 'hg', 'wt'.

To show principal dimensions that are the most associated with clusters:

```{r, include=TRUE}
res.hcpc$desc.axes$quanti
```

The above output indicates that:
* individuals in cluster 1 have high coordinates on axes 1 and negative ones on axes 2
* individuals in cluster 2 have negative coordinates on axes 2
* individuals in cluster 3 have positive coordinates on axes 1 and 2

Representative individuals of each cluster can be extracted as follows:

```{r, include=TRUE}
res.hcpc$desc.ind$para
```

For each cluster, the top 5 closest individuals to the cluster center is shown. The distance between each individual and the cluster center is provided. For example, representative individuals for cluster 1 include individuals number 41, 8, 5, 32, 31

To visualize the dendrogram generated by the hierarchical clustering, we use the function _fviz_dend_ of the _factoextra_ package:

```{r, include=TRUE}
fviz_dend(res.hcpc, 
          cex = 0.7,            # Label size
          palette = "jco",      # Color palette 
          rect = TRUE, 
          rect_fill = TRUE, # Add rectangle around groups
          rect_border = "jco",           # Rectangle color
          labels_track_height = 0.8   # Increase the room for labels
          )
```

As we did above, it is possible to visualize individuals on the principal component map and to color individuals according to the cluster they belong to. The function _fviz_cluster_ (package _factoextra_) can be used to visualize individuals clusters:

```{r, include=TRUE}
fviz_cluster(res.hcpc,
             repel = TRUE,            # Avoid label overlapping
             show.clust.cent = TRUE, # Show cluster centers
             palette = "jco",     # Color palette see 
             ggtheme = theme_minimal(),
             main = "Factor map"
             )
```

What we can see from the above plot is that the HCPC method performs a quite good separation of the individuals on 3 clusters, which is perfectly described by the first factorial plane.

We can also combine PCA and tree and depict a 3D plot combining the hierarchical clustering and the factorial map using the R base function _plot_:

```{r, include=TRUE}
plot(res.hcpc, choice = "3D.map")
```


# Partitioning clustering: k-means

Let's now focus on the most famous partitioning clustering algorithm: k-means. It's a stable algorithm based on moving averages where, at each steps, elements are gathered into the closest group (in terms of Euclidean distance for example). 
Because the algorithm is based on arbitrary starting centroids, we will run it multiple times in order to make the most generalist conclusions possible about our data.
To compute this algorithm, we will use the function *kmeans* from the R package _stats_.

## Finding the number of clusters
In order to have the most relevant method, we first need to find the optimal number of clusters (which is the number of centroids we will start with). We will use various methods:
* Elbow method: based on plotting the within-cluster sum of squares (wss) and then identifying the optimal number of clusters (or centers) at the bend (called elbow) of the plot.
* Silhouette method: based on how well each object lies within a cluster.
* Gap statistic method: based on a null hypothesis (statistical approach), compares the total within-cluster variation using different k values on various samples of the data, using bootstrapping.

```{r, include=TRUE}
# set seed for repeating experiments
set.seed(67600)
```

Elbow method:
```{r, include=TRUE}
fviz_nbclust(Prostate_no_outliers_sc, kmeans, method = "wss") +
geom_vline(xintercept = 2, linetype = 2)
```

Average silhouette:
```{r, include=TRUE}
fviz_nbclust(Prostate_no_outliers_sc, kmeans, method = "silhouette")
```

Gap statistics:
```{r, include=TRUE}
# we used B = ~500
gap_stat <- clusGap(Prostate_no_outliers_sc, FUN = kmeans, 
                    nstart = 25, K.max = 10, B = 500)
fviz_gap_stat(gap_stat)
```

A more robust way could be to use the R package and function _NBclust_, which provides 30 indexes for determining the number of clusters and proposes to use the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods:

```{r, include=TRUE}
library(NbClust)
nc <- NbClust(Prostate_no_outliers_sc, min.nc=2, max.nc=10, method="kmeans")
```

We'll show the clusters for k=2 and k=3, which is the optimal value suggested by _NBclust_.

```{r, include=TRUE}
k2 <- kmeans(Prostate_no_outliers_sc, centers = 2, nstart = 25)
k3 <- kmeans(Prostate_no_outliers_sc, centers = 3, nstart = 25)
```

We create the graphs side-by-side:

```{r, include=TRUE}
p1 <- fviz_cluster(k2, geom = "point", data = Prostate_no_outliers_sc) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = Prostate_no_outliers_sc) + ggtitle("k = 3")

grid.arrange(p1, p2, nrow = 1)
```

From the plot above, we can graphically visualize the 2 explored cases and it is clearly visible that the case with k=2 better separates the observations, without any overlapping.
Since most of the methods suggested the case with k=3 (which is also the value we obtained in the hierarchical algorithms), let's do some more in-depth analysis to understand the meaning of the obtained clusters.

We can compute the mean of each variables by cluster using the original data:

```{r, include=TRUE}
aggregate(Prostate_no_outliers_sc, by=list(cluster=k3$cluster), mean) %>% 
    kbl(caption = "Mean by group and by variable") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

From the above table we can see the following:
* Cluster 1 contains observations with (on average) negative values along all the variables.
* Cluster 2 has positive averages for each variable, except for the last 3.
* Cluster 3 contains elements characterized by high values of the variables 'sz', 'sg' and 'ap'.

Let's check the cluster size:
```{r, include=TRUE}
k3$size
```

and the graph:

```{r, include=TRUE}
fviz_cluster(k3,  data = Prostate_no_outliers_sc) + ggtitle("k = 3")
```


# Hierarchical K-Means Clustering

K-means, as we know, is very sensitive to the initial choices made regarding the number of clusters we want to find and to the initial positioning of the centroids.
Therefore, one possible improvement that can be made is to integrate a hierarchical clustering approach in the initial stage of the k-means algorithm, to better initialize the centroids when applying k-means.
This is what the hierarchical k-means clustering algorithm does, allowing to obtain more robust clusters with respect to the classical k-means.

Let's apply the algorithm:

```{r, include=TRUE}
res.hk <-hkmeans(Prostate_no_outliers_sc,3)
```

Print the results:

```{r, include=TRUE}
res.hk
```

Visualize the dendrogram:

```{r, include=TRUE}
fviz_dend(res.hk, cex = 0.6, palette = "jco", 
          rect = TRUE, rect_border = "jco", rect_fill = TRUE)
```

Visualize the final clusters:
```{r, include=TRUE}
fviz_cluster(res.hk, palette = "jco", repel = TRUE,
             ggtheme = theme_classic())
```

Let's visualize one beside the other the results of k-means and the hierarchical k-means we just performed, to visualize the eventual differences:

```{r}
g1 <- fviz_cluster(k3,  data = Prostate_no_outliers_sc, palette = "jco", repel = TRUE)
g2 <- fviz_cluster(res.hk, palette = "jco", repel = TRUE)
grid.arrange(g1, g2)
```

The plot shows that the introduction of the hierachical clustering initialization in the k-means improves the separation between the clusters already found with classical k-means. Indeed the 3 groups are more separated in the 2nd case.

# Model-based clustering

In model-based clustering, the data is considered as coming from a mixture of distributions. 

In our case, we consider a Gaussian mixture model, where each component k in the model can be considered as a cluster and is modeled by a normal distribution

Model-based clustering doesn't require to do a standardization, but we'll use the standardized data to be able to compare the results of the different algorithms.

We'll use the R package *mclust* which estimates the model's parameters by EM algorithm initialized by hierarchical model-based agglomerative clustering.
The optimal model is then selected according to the BIC criteria: 

The function we'll use describes the obtained model with a character string composed by 3 characters indicating the characteristics of the model at which the optimal BIC occurs: 
* the first character refers to volume
* the second to shape
* the third to orientation

In details: character E stands for "equal", V for "variable" and I for "coordinate axes".
The available models are those reported in the following picture:
![](mclust.png)

Let's now load the library of interest:

```{r, include=TRUE}
library(mclust)
```
We can now perform the fitting and print a summary of the obtained model:

```{r, include=TRUE}
mc <- Mclust(Prostate_no_outliers_sc)
summary(mc) # Print a summary
```

The lowest the BIC, the better the model fits to the data: in this case we have a strongly negative BIC.
The above output shows that the optimal model returned by the algorithm  is composed by 2 clusters with respectively 28 and 18 observations. We therefore obtained a different result with respect to the previously considered methods which return 3 clusters in most of the cases.
Ww think this is a more realistic scenario, considering that our data is about patients that have either stage 3 or stage 4 prostate cancer: therefore we suppose that the found clusters separate people with this two cancer stages.
The optimal selected model name is a VVI model (see picture above): that is the 2 spotted clusters are ellipsoidal with varying volume, varying shape and the orientation is the identity (I) or "coordinate axes'". 

To better understand the results of the algorithm, we can analyze one by one the outputs of the *Mclust* function:

Name of the optimal model:
```{r, include=TRUE}
mc$modelName     
```
Number of clusters in the optimal model:
```{r, include=TRUE}
mc$G     
```
Probability to belong to a given cluster:
```{r, include=TRUE}
mc$z  %>%
    as.data.frame(.) %>% 
  sample_n(., 10, replace=FALSE) %>% 
  kbl(caption = "Probability to belong to a given cluster (sample of 10)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

We know that model-based clustering is a soft-clustering technique: therefore we have for each sample a probability to belong to each one of the clusters.
The above table, shows that for this sample of data, the assignment of points to clusters is pretty strong, since we have high probabilities for each observation to belogn to a single specific cluster.

We can also visualize the cluster allocation of each observation:
```{r, include=TRUE}                    
head(mc$classification,10) # first 10 observations
```

We can use the function  _plot.Mclust_ in the  *mclust* package to plot the results of the algorithm. Some graphs can be plotted by using the *factoextra* package, which uses _ggplot2_ providing a better plotting style.

We can  graph the *BIC values* used for choosing the number of clusters:
```{r, include=TRUE}
library(factoextra)
fviz_mclust(mc, "BIC", palette = "jco")
```
We can modify the above graph to show only the final optimal model we are interested in using the function *fviz_mclust_bic*:

```{r, include=TRUE}
library(factoextra)
fviz_mclust_bic(mc, model.names=mc$modelName, palette = "jco")
```

From *mclust*, we can obtain the top BIC ranking:

```{r, include=TRUE}
summary(mc$BIC)
```

By specifying the 'classification' paramerer we chan visualize a plot showing the clustering. 
```{r, include=TRUE}
plot.Mclust(mc, what="classification", addEllipses = FALSE)
``` 

Ellipses corresponding to covariances of mixture components are also drawn if _addEllipses = TRUE_.:
```{r, include=TRUE}
plot.Mclust(mc, what="classification", addEllipses = TRUE)
``` 
The above plots are not very clear due to the high number of variables used for performing the clustering: indeed they show the found cluster in the graph with each group of 2 variables. The above plots show for instance that variables 'sg' and 'ap' are very discriminatory for the separation of points into the 2 groups.

We may prefer to show the clustering result in the first factorial plane (obtained through PCA). To do that we can use the *factoextra* package wich performs a PCA to show the results in the first 2 PCs:  

```{r, include=TRUE}
fviz_mclust(mc, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco")
```
In the 1st factorial plane the 2 clusters are pretty well separated by the diagonal of the 1st quadrant.


We can now plot the classification uncertainty:

```{r, include=TRUE}
plot.Mclust(mc, what="uncertainty")
``` 
In this plot, _larger symbols indicate the more uncertain observations_. The package *factoextra* provides a better visualization for classification uncertainty on the first factorial plane (first 2 PCs):

```{r, include=TRUE}
fviz_mclust(mc, "uncertainty", palette = "jco")
```
The plot shows that there are 2 points outside the 2 represented densities, with one of them (around poisition (2,2)) showed with high size, indicating a pretty strong uncertainty in its clusters association.


We can also visaulize a plot of the estimated density: 
```{r, include=TRUE}
plot.Mclust(mc, what="density", type = "image", col="steelblue", grid = 200)
```
We can also show 3D graphs of the estimated denisities:
```{r, include=TRUE}
plot.Mclust(mc, what="density", type="persp")
```
The density estimation shows the estimated densities by considering eahc pair of variables: we can see that only some planes show 2 estimated Gaussians, while the planes of some variables are not able to discriminate the 2 clusters. For instance, all the plots cosidering variable 'sg' show very clearly the 2 densities.


For instance, to better show the results we discussed above, we may plot the data using only two variables of interest: we can select 'sg' as one of them (since, as said before, it discriminates very well the 2 groups) and 'dbp':

```{r, include=TRUE}
# Classification: plot showing the clustering
fviz_mclust(mc, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco", 
            choose.vars = c("sg", "dbp"))
```
As expected, these 2 variables are pretty good in discrimating the 2 groups.



It is possible to use also another dimensionality reduction method for visualizing the clustering structure obtained from a finite mixture of Gaussian densities with the function *MclustDR*. 

The estimated directions which span the reduced subspace are defined as a set of *linear combinations of the original features, ordered by importance as quantified by the associated eigenvalues*.


```{r, include=TRUE}
drmc <- MclustDR(mc, lambda = 1)
summary(drmc)
```
In our case, the above method reduced the data to only 1 direction, which captures most of the clustering structure found with the model-based algorihm.


The projected plot on this new direction would be:
```{r, include=TRUE}
plot(drmc, what = "contour")
```
The plot allows to visualize, with a sort of box-plot, also the uncertainty in assigning the obsevations into the 2 clusters.
The box-plots show also the values in the new direction (Dir1) of the observations in each cluster, and we can see that the data are very well classified into 2 groups.

We can also visualize the density:

```{r, include=TRUE}
plot(drmc, what = "density")
```

Save image of all objects created in the session:
```{r, include=TRUE}
save.image(file="Clustering.RData")
```